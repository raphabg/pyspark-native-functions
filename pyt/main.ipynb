{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fuzzy Match POC with Apache Spark\n",
    "The objective of this project is to test the execution of native spark functions to perform string similarity analysis, with variated similarity analysis algorithms\n",
    "\n",
    "### Approaches\n",
    "\n",
    "- 1st Approach: Use of native Scala Spark SQL fuzzy match algorithms, crossjoining the input dataset with the target dataset, generating a quatratic computational time\n",
    "- 2nd Approach: Use of Term Frequency, Inverse Document Frequency (TF-IDF) and only then applying native Scala Spark SQL fuzzy match algorithms\n",
    "\n",
    "References:\n",
    "- [Josh Taylor: Fuzzy matching at scale](https://towardsdatascience.com/fuzzy-matching-at-scale-84f2bfd0c536#:~:text=The%20problem%20with%20Fuzzy%20Matching%20on%20large%20data&text=In%20computer%20science%2C%20this%20is,that%20works%20in%20quadratic%20time.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import names\n",
    "import pyspark.sql.functions as F\n",
    "import numpy as np\n",
    "import sparse_dot_topn.sparse_dot_topn as ct\n",
    "import pandas as pd\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "sc = SparkContext().getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder.appName(\n",
    "    'Fuzzy Match POC').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare datasets\n",
    "`targets` is the name list that will be looked up inside `comparison`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = ['A', 'E', 'I', 'O', 'U', 'H' , 'R', 'P' ,'B', 'J', 'N', 'M', 'G', '.', '/', './ ', '-', '- LTDA']\n",
    "targets = []\n",
    "comparison = []\n",
    "\n",
    "targets.append({'SimilarityWith': 'HP PARTICIPACOES S/A'})\n",
    "comparison.append({'Name': 'HAP PARCTICIPACOES S/A'})\n",
    "\n",
    "for i in range(2):\n",
    "    name = names.get_full_name().upper()\n",
    "    targets.append({'SimilarityWith': name})\n",
    "    comparison.append({'Name': name})\n",
    "\n",
    "    #replace shuffle\n",
    "    for shuffle_char in shuffle:\n",
    "        for shuffle_char_aux in shuffle:\n",
    "            if shuffle_char != shuffle_char_aux:\n",
    "                shuffled_name = name.replace(shuffle_char, shuffle_char_aux)\n",
    "                comparison.append({'Name': shuffled_name})\n",
    "\n",
    "targets_df = spark.createDataFrame(targets).alias('t')\n",
    "comparison_df = spark.createDataFrame(comparison).alias('c')\n",
    "\n",
    "print(len(targets))\n",
    "print(len(comparison))\n",
    "\n",
    "print(f\"CrossJoined dataset size: {len(targets) * len(comparison)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Conventional CrossJoin Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lev = F.expr('1 - (levenshtein(Name, SimilarityWith) / array_max(array(length(Name), length(SimilarityWith))) )')\n",
    "jaro = F.expr('jarowinkler(Name, SimilarityWith)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = targets_df.crossJoin(comparison_df)\n",
    "df = df.withColumn('Similarity', (lev + jaro) / 2).filter(F.expr('Similarity > 0.8')).cache()\n",
    "\n",
    "print(f\"Filtered Fuzzy Match count: {df.count()}\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = df.toPandas()\n",
    "pandas_df.to_csv('conventional.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "### Term Frequency, Inverse Document Frequency (TF-IDF) Approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Function that generates list of 3 char length ngrams from full string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(string, n=2):\n",
    "    ngs = zip(*[string[i:] for i in range(n)])\n",
    "    return [''.join(n) for n in ngs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_list = list(set(map(lambda x: x['SimilarityWith'], targets)))\n",
    "comparison_list = list(set(map(lambda x: x['Name'], comparison)))\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams, lowercase=False)\n",
    "tfidf = vectorizer.fit_transform(targets_list)\n",
    "nbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tfidf)\n",
    "\n",
    "def getNearestN(query):\n",
    "  queryTFIDF_ = vectorizer.transform(query)\n",
    "  distances, indices = nbrs.kneighbors(queryTFIDF_)\n",
    "  return distances, indices\n",
    "\n",
    "distances, indices = getNearestN(comparison_list)\n",
    "comparison_list = list(comparison_list)\n",
    "\n",
    "matches = []\n",
    "for i,j in enumerate(indices):\n",
    "  temp = [round(distances[i][0],2), targets_list[j[0]], comparison_list[i]]\n",
    "  matches.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(matches, columns=['Distance','Target name','Dataset name'])\n",
    "\n",
    "#matches_df = matches_df.loc[matches_df['Distance'] > 0.3]\n",
    "matches_df = matches_df.drop_duplicates().sort_values(\n",
    "    by=['Distance'], ascending=True)\n",
    "    \n",
    "matches_df.to_csv('tf-idf.csv', index=False)\n",
    "print(len(matches_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarks\n",
    "\n",
    "|Approach|Search Targets Count|Dataset Names Count|CrossJoined Dataset Size| Hit Count | Score Filter | Duration |\n",
    "|--|--|--|--|--|--|--|\n",
    "|Conventional|10.000|126.304|1.263.040.000|199.549|80% > |~3m 11s (4m 41s total)|\n",
    "|TFIDF|10.000|126.304|-|91.622|0.3 >|15.7s|\n",
    "|TFIDF|10.000|126.304|-|126.304|N/A|15s|\n",
    "|TFIDF|5.000|63.635|-|46.411|0.3 >|4.6s|\n",
    "|Conventional|5.000|63.635|318.175.000|82.011|80% >|~31s (2m 1s total)|\n",
    "\n",
    "* Spark seems to take about 1m 30s initialization time independently of the dataset size, at the computer i'm currently running the script"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5e111dc883ad0d35b8f985fc39843362d6c339b3768984b0dcc929159298b881"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
