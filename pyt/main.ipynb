{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "interpreter": {
            "hash": "7257c864f8597f6c2ca5a7cd46c52e40d71c8d4741abce4a0beb54f0b0fd7315"
        },
        "orig_nbformat": 4
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Fuzzy Match POC with Apache Spark\n",
                "The objective of this project is to test the execution of native spark functions to perform string similarity analysis, with variated similarity analysis algorithms\n",
                "\n",
                "### Approaches\n",
                "\n",
                "- 1st Approach: Use of native Scala Spark SQL fuzzy match algorithms, crossjoining the input dataset with the target dataset, generating a quatratic computational time\n",
                "- 2nd Approach: Use of Term Frequency, Inverse Document Frequency (TF-IDF) and only then applying native Scala Spark SQL fuzzy match algorithms\n",
                "\n",
                "References:\n",
                "- [Josh Taylor: Fuzzy matching at scale](https://towardsdatascience.com/fuzzy-matching-at-scale-84f2bfd0c536#:~:text=The%20problem%20with%20Fuzzy%20Matching%20on%20large%20data&text=In%20computer%20science%2C%20this%20is,that%20works%20in%20quadratic%20time.)"
            ],
            "metadata": {
                "azdata_cell_guid": "4aeacaa4-901d-441f-a8e8-4c1bae28c774"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "import os\n",
                "import sys\n",
                "import names\n",
                "import pyspark.sql.functions as F\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pyspark import SparkContext\n",
                "from pyspark.sql import SparkSession\n",
                "from scipy.sparse import csr_matrix\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
                "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
                "\n",
                "sc = SparkContext().getOrCreate()\n",
                "\n",
                "spark = SparkSession.builder.appName(\n",
                "    'Fuzzy Match POC').getOrCreate()"
            ],
            "metadata": {
                "azdata_cell_guid": "a7291e39-9e81-4859-868e-713730d26357",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "error",
                    "ename": "ValueError",
                    "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at C:\\Users\\Raphael\\AppData\\Local\\Temp\\ipykernel_15924\\1014550859.py:17 ",
                    "traceback": [
                        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
                        "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYSPARK_PYTHON\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexecutable\n\u001b[0;32m     15\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPYSPARK_DRIVER_PYTHON\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mexecutable\n\u001b[1;32m---> 17\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m     19\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFuzzy Match POC\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyspark\\context.py:144\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 144\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n\u001b[0;32m    147\u001b[0m                   conf, jsc, profiler_cls)\n",
                        "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\pyspark\\context.py:350\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    347\u001b[0m     callsite \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\u001b[38;5;241m.\u001b[39m_callsite\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;66;03m# Raise error if there is already a running Spark context\u001b[39;00m\n\u001b[1;32m--> 350\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    351\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot run multiple SparkContexts at once; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    352\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexisting SparkContext(app=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, master=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    353\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m created by \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m at \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    354\u001b[0m         \u001b[38;5;241m%\u001b[39m (currentAppName, currentMaster,\n\u001b[0;32m    355\u001b[0m             callsite\u001b[38;5;241m.\u001b[39mfunction, callsite\u001b[38;5;241m.\u001b[39mfile, callsite\u001b[38;5;241m.\u001b[39mlinenum))\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    357\u001b[0m     SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context \u001b[38;5;241m=\u001b[39m instance\n",
                        "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at C:\\Users\\Raphael\\AppData\\Local\\Temp\\ipykernel_15924\\1014550859.py:17 "
                    ]
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Prepare datasets\n",
                "`targets` is the name list that will be looked up inside `comparison`"
            ],
            "metadata": {
                "azdata_cell_guid": "277c556d-9166-4088-8151-b357c4b6c11b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "shuffle = ['A', 'E', 'I', 'O', 'U', 'H' , 'R', 'P' ,'B', 'J', 'N', 'M', 'G', '.', '/', './ ', '-', '- LTDA']\n",
                "targets = []\n",
                "comparison = []\n",
                "\n",
                "targets.append({'SimilarityWith': 'HP PARTICIPACOES S/A'})\n",
                "comparison.append({'Name': 'HAP PARCTICIPACOES S/A'})\n",
                "\n",
                "for i in range(2):\n",
                "    name = names.get_full_name().upper()\n",
                "    targets.append({'SimilarityWith': name})\n",
                "    comparison.append({'Name': name})\n",
                "\n",
                "    #replace shuffle\n",
                "    for shuffle_char in shuffle:\n",
                "        for shuffle_char_aux in shuffle:\n",
                "            if shuffle_char != shuffle_char_aux:\n",
                "                shuffled_name = name.replace(shuffle_char, shuffle_char_aux)\n",
                "                comparison.append({'Name': shuffled_name})\n",
                "\n",
                "targets_df = spark.createDataFrame(targets).alias('t')\n",
                "comparison_df = spark.createDataFrame(comparison).alias('c')\n",
                "\n",
                "print(len(targets))\n",
                "print(len(comparison))\n",
                "\n",
                "print(f\"CrossJoined dataset size: {len(targets) * len(comparison)}\")"
            ],
            "metadata": {
                "azdata_cell_guid": "ed14ad64-b015-4c66-b253-68551dcf4cba",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "3\n615\nCrossJoined dataset size: 1845\nCPU times: total: 31.2 ms\nWall time: 1.37 s\n"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "<hr>\n",
                "\n",
                "### Conventional CrossJoin Approach"
            ],
            "metadata": {
                "azdata_cell_guid": "3c765323-b6ec-4a55-a3ca-db75d3ef8228"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "lev = F.expr('1 - (levenshtein(Name, SimilarityWith) / array_max(array(length(Name), length(SimilarityWith))) )')\n",
                "jaro = F.expr('jarowinkler(Name, SimilarityWith)')\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "6e021e0f-a1a3-4c45-8c87-f45d47c59b93",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "CPU times: total: 0 ns\nWall time: 7.01 ms\n"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "df = targets_df.crossJoin(comparison_df)\n",
                "df = df.withColumn('Similarity', (lev + jaro) / 2).filter(F.expr('Similarity > 0.8')).cache()\n",
                "\n",
                "print(f\"Filtered Fuzzy Match count: {df.count()}\")\n",
                "df.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "c7ce4e17-0f42-4a21-bb57-371a819cb799",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Filtered Fuzzy Match count: 598\n+--------------------+--------------------+------------------+\n|      SimilarityWith|                Name|        Similarity|\n+--------------------+--------------------+------------------+\n|HP PARTICIPACOES S/A|HAP PARCTICIPACOE...|0.8734090909090909|\n|         MICHAEL AKE|         MICHAEL AKE|               1.0|\n|         MICHAEL AKE|         MICHEEL EKE|0.8616161616161615|\n|         MICHAEL AKE|         MICHIEL IKE|0.8727272727272728|\n|         MICHAEL AKE|         MICHOEL OKE|0.8727272727272728|\n|         MICHAEL AKE|         MICHUEL UKE|0.8727272727272728|\n|         MICHAEL AKE|         MICHHEL HKE|0.8727272727272728|\n|         MICHAEL AKE|         MICHREL RKE|0.8727272727272728|\n|         MICHAEL AKE|         MICHPEL PKE|0.8727272727272728|\n|         MICHAEL AKE|         MICHBEL BKE|0.8727272727272728|\n|         MICHAEL AKE|         MICHJEL JKE|0.8727272727272728|\n|         MICHAEL AKE|         MICHNEL NKE|0.8727272727272728|\n|         MICHAEL AKE|         MICHMEL MKE|0.8727272727272728|\n|         MICHAEL AKE|         MICHGEL GKE|0.8727272727272728|\n|         MICHAEL AKE|         MICH.EL .KE|0.8727272727272728|\n|         MICHAEL AKE|         MICH/EL /KE|0.8727272727272728|\n|         MICHAEL AKE|         MICH-EL -KE|0.8727272727272728|\n|         MICHAEL AKE|         MICHAAL AKA| 0.856060606060606|\n|         MICHAEL AKE|         MICHAIL AKI|0.8727272727272728|\n|         MICHAEL AKE|         MICHAOL AKO|0.8727272727272728|\n+--------------------+--------------------+------------------+\nonly showing top 20 rows\n\nCPU times: total: 15.6 ms\nWall time: 1min 33s\n"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "pandas_df = df.toPandas()\n",
                "pandas_df.to_csv('conventional.csv', index=False)"
            ],
            "metadata": {
                "azdata_cell_guid": "af65312e-df81-4f75-81dd-b41e5caddfba",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "CPU times: total: 109 ms\nWall time: 332 ms\n"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "<hr>\n",
                "\n",
                "### Term Frequency, Inverse Document Frequency (TF-IDF) Approach "
            ],
            "metadata": {
                "azdata_cell_guid": "04f98435-9898-4c98-ac72-b0a8b56d22b2"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "Function that generates list of 3 char length ngrams from full string "
            ],
            "metadata": {
                "azdata_cell_guid": "42e58380-fcb7-495c-b08e-75654ec29bde"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "def ngrams(string, n=2):\n",
                "    ngs = zip(*[string[i:] for i in range(n)])\n",
                "    return [''.join(n) for n in ngs]"
            ],
            "metadata": {
                "azdata_cell_guid": "b3d1418c-376a-453e-bedf-6ab1335003ab",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "CPU times: total: 0 ns\nWall time: 0 ns\n"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "targets_list = list(set(map(lambda x: x['SimilarityWith'], targets)))\n",
                "comparison_list = list(set(map(lambda x: x['Name'], comparison)))\n",
                "\n",
                "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams, lowercase=False)\n",
                "tfidf = vectorizer.fit_transform(targets_list)\n",
                "nbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tfidf)\n",
                "\n",
                "def getNearestN(query):\n",
                "  queryTFIDF_ = vectorizer.transform(query)\n",
                "  distances, indices = nbrs.kneighbors(queryTFIDF_)\n",
                "  return distances, indices\n",
                "\n",
                "distances, indices = getNearestN(comparison_list)\n",
                "comparison_list = list(comparison_list)\n",
                "\n",
                "matches = []\n",
                "for i,j in enumerate(indices):\n",
                "  temp = [round(distances[i][0],2), targets_list[j[0]], comparison_list[i]]\n",
                "  matches.append(temp)"
            ],
            "metadata": {
                "azdata_cell_guid": "342ba2e9-9954-49fe-87e1-bb5df436689a",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "CPU times: total: 15.6 ms\nWall time: 22 ms\n"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "matches_df = pd.DataFrame(matches, columns=['Distance','Target name','Dataset name'])\n",
                "\n",
                "#matches_df = matches_df.loc[matches_df['Distance'] > 0.3]\n",
                "matches_df = matches_df.drop_duplicates().sort_values(\n",
                "    by=['Distance'], ascending=True)\n",
                "    \n",
                "matches_df.to_csv('tf-idf.csv', index=False)\n",
                "print(len(matches_df))"
            ],
            "metadata": {
                "azdata_cell_guid": "4f75fa7e-f71e-46d4-8869-dac3feb7d367",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "224\nCPU times: total: 15.6 ms\nWall time: 5.02 ms\n"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "markdown",
            "source": [
                "Benchmarks\n",
                "\n",
                "|Approach|Search Targets Count|Dataset Names Count|CrossJoined Dataset Size| Hit Count | Score Filter | Duration |\n",
                "|--|--|--|--|--|--|--|\n",
                "|Conventional|5.000|785.000|3.925.000.000|1.257.568|80% > |~11m (12m 30s)|\n",
                "|TFIDF|5.000|785.000|-|728.326|0.3 >|56s|\n",
                "|Conventional|10.000|126.304|1.263.040.000|199.549|80% > |~3m 11s (4m 41s total)|\n",
                "|TFIDF|10.000|126.304|-|91.622|0.3 >|15.7s|\n",
                "|TFIDF|10.000|126.304|-|126.304|N/A|15s|\n",
                "|TFIDF|5.000|63.635|-|46.411|0.3 >|4.6s|\n",
                "|Conventional|5.000|63.635|318.175.000|82.011|80% >|~31s (2m 1s total)|\n",
                "\n",
                "* Spark seems to take about 1m 30s initialization time independently of the dataset size, at the computer i'm currently running the script"
            ],
            "metadata": {
                "azdata_cell_guid": "8830ad1d-74e4-48f5-bbc4-fa680cb970fe"
            }
        }
    ]
}