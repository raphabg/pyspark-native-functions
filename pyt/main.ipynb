{
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3 (ipykernel)",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "interpreter": {
            "hash": "7257c864f8597f6c2ca5a7cd46c52e40d71c8d4741abce4a0beb54f0b0fd7315"
        },
        "orig_nbformat": 4
    },
    "nbformat_minor": 2,
    "nbformat": 4,
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Fuzzy Match POC with Apache Spark\n",
                "The objective of this project is to test the execution of native spark functions to perform string similarity analysis, with variated similarity analysis algorithms\n",
                "\n",
                "### Approaches\n",
                "\n",
                "- 1st Approach: Use of native Scala Spark SQL fuzzy match algorithms, crossjoining the input dataset with the target dataset, generating a quatratic computational time\n",
                "- 2nd Approach: Use of Term Frequency, Inverse Document Frequency (TF-IDF) and only then applying native Scala Spark SQL fuzzy match algorithms\n",
                "\n",
                "References:\n",
                "- [Josh Taylor: Fuzzy matching at scale](https://towardsdatascience.com/fuzzy-matching-at-scale-84f2bfd0c536#:~:text=The%20problem%20with%20Fuzzy%20Matching%20on%20large%20data&text=In%20computer%20science%2C%20this%20is,that%20works%20in%20quadratic%20time.)"
            ],
            "metadata": {
                "azdata_cell_guid": "4aeacaa4-901d-441f-a8e8-4c1bae28c774"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import names\n",
                "import pyspark.sql.functions as F\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pyspark import SparkContext\n",
                "from pyspark.sql import SparkSession\n",
                "from scipy.sparse import csr_matrix\n",
                "from sklearn.metrics.pairwise import cosine_similarity\n",
                "from sklearn.neighbors import NearestNeighbors\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "\n",
                "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
                "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
                "\n",
                "sc = SparkContext().getOrCreate()\n",
                "\n",
                "spark = SparkSession.builder.appName(\n",
                "    'Fuzzy Match POC').getOrCreate()"
            ],
            "metadata": {
                "azdata_cell_guid": "a7291e39-9e81-4859-868e-713730d26357",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "CPU times: total: 2.83 s\nWall time: 15.3 s\n"
                }
            ],
            "execution_count": 2
        },
        {
            "cell_type": "markdown",
            "source": [
                "### Prepare datasets\n",
                "`targets` is the name list that will be looked up inside `comparison`"
            ],
            "metadata": {
                "azdata_cell_guid": "277c556d-9166-4088-8151-b357c4b6c11b"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "shuffle = ['A', 'E', 'I', 'O', 'U', 'H' , 'R', 'P' ,'B', 'J', 'N', 'M', 'G', '.', '/', './ ', '-', '- LTDA']\n",
                "targets = []\n",
                "comparison = []\n",
                "\n",
                "targets.append({'SimilarityWith': 'HP PARTICIPACOES S/A'})\n",
                "comparison.append({'Name': 'HAP PARCTICIPACOES S/A'})\n",
                "\n",
                "for i in range(2):\n",
                "    name = names.get_full_name().upper()\n",
                "    targets.append({'SimilarityWith': name})\n",
                "    comparison.append({'Name': name})\n",
                "\n",
                "    #replace shuffle\n",
                "    for shuffle_char in shuffle:\n",
                "        for shuffle_char_aux in shuffle:\n",
                "            if shuffle_char != shuffle_char_aux:\n",
                "                shuffled_name = name.replace(shuffle_char, shuffle_char_aux)\n",
                "                comparison.append({'Name': shuffled_name})\n",
                "\n",
                "targets_df = spark.createDataFrame(targets).alias('t')\n",
                "comparison_df = spark.createDataFrame(comparison).alias('c')\n",
                "\n",
                "print(len(targets))\n",
                "print(len(comparison))\n",
                "\n",
                "print(f\"CrossJoined dataset size: {len(targets) * len(comparison)}\")"
            ],
            "metadata": {
                "azdata_cell_guid": "ed14ad64-b015-4c66-b253-68551dcf4cba",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "3\n615\nCrossJoined dataset size: 1845\nCPU times: total: 78.1 ms\nWall time: 2.25 s\n"
                }
            ],
            "execution_count": 3
        },
        {
            "cell_type": "markdown",
            "source": [
                "<hr>\n",
                "\n",
                "### Conventional CrossJoin Approach"
            ],
            "metadata": {
                "azdata_cell_guid": "3c765323-b6ec-4a55-a3ca-db75d3ef8228"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "lev = F.expr('1 - (levenshtein(Name, SimilarityWith) / array_max(array(length(Name), length(SimilarityWith))) )')\n",
                "jaro = F.expr('bis_brasil_jarowinkler(Name, SimilarityWith)')\n",
                ""
            ],
            "metadata": {
                "azdata_cell_guid": "6e021e0f-a1a3-4c45-8c87-f45d47c59b93",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "CPU times: total: 0 ns\nWall time: 102 ms\n"
                }
            ],
            "execution_count": 4
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "df = targets_df.crossJoin(comparison_df)\n",
                "df = df.withColumn('Similarity', (lev + jaro) / 2).filter(F.expr('Similarity > 0.8')).cache()\n",
                "\n",
                "print(f\"Filtered Fuzzy Match count: {df.count()}\")\n",
                "df.show()"
            ],
            "metadata": {
                "azdata_cell_guid": "c7ce4e17-0f42-4a21-bb57-371a819cb799",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "Filtered Fuzzy Match count: 602\n"
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "+--------------------+--------------------+------------------+\n|      SimilarityWith|                Name|        Similarity|\n+--------------------+--------------------+------------------+\n|HP PARTICIPACOES S/A|HAP PARCTICIPACOE...|0.8734090909090909|\n|          KAREN HILL|          KAREN HILL|               1.0|\n|          KAREN HILL|          KEREN HILL|0.9033333333333333|\n|          KAREN HILL|          KIREN HILL|0.9199999999999999|\n|          KAREN HILL|          KOREN HILL|0.9199999999999999|\n|          KAREN HILL|          KUREN HILL|0.9199999999999999|\n|          KAREN HILL|          KHREN HILL|0.9199999999999999|\n|          KAREN HILL|          KRREN HILL|0.9199999999999999|\n|          KAREN HILL|          KPREN HILL|0.9199999999999999|\n|          KAREN HILL|          KBREN HILL|0.9199999999999999|\n|          KAREN HILL|          KJREN HILL|0.9199999999999999|\n|          KAREN HILL|          KNREN HILL|             0.895|\n|          KAREN HILL|          KMREN HILL|0.9199999999999999|\n|          KAREN HILL|          KGREN HILL|0.9199999999999999|\n|          KAREN HILL|          K.REN HILL|0.9199999999999999|\n|          KAREN HILL|          K/REN HILL|0.9199999999999999|\n|          KAREN HILL|          K-REN HILL|0.9199999999999999|\n|          KAREN HILL|          KARAN HILL|0.9266666666666666|\n|          KAREN HILL|          KARIN HILL|0.9007407407407408|\n|          KAREN HILL|          KARON HILL|0.9266666666666666|\n+--------------------+--------------------+------------------+\nonly showing top 20 rows\n\nCPU times: total: 125 ms\nWall time: 3min 11s\n"
                }
            ],
            "execution_count": 5
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "pandas_df = df.toPandas()\n",
                "pandas_df.to_csv('conventional.csv', index=False)"
            ],
            "metadata": {
                "azdata_cell_guid": "af65312e-df81-4f75-81dd-b41e5caddfba",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "CPU times: total: 109 ms\nWall time: 332 ms\n"
                }
            ],
            "execution_count": 6
        },
        {
            "cell_type": "markdown",
            "source": [
                "<hr>\n",
                "\n",
                "### Term Frequency, Inverse Document Frequency (TF-IDF) Approach "
            ],
            "metadata": {
                "azdata_cell_guid": "04f98435-9898-4c98-ac72-b0a8b56d22b2"
            }
        },
        {
            "cell_type": "markdown",
            "source": [
                "\n",
                "Function that generates list of 3 char length ngrams from full string "
            ],
            "metadata": {
                "azdata_cell_guid": "42e58380-fcb7-495c-b08e-75654ec29bde"
            }
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "def ngrams(string, n=2):\n",
                "    ngs = zip(*[string[i:] for i in range(n)])\n",
                "    return [''.join(n) for n in ngs]"
            ],
            "metadata": {
                "azdata_cell_guid": "b3d1418c-376a-453e-bedf-6ab1335003ab",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "CPU times: total: 0 ns\nWall time: 0 ns\n"
                }
            ],
            "execution_count": 7
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "targets_list = list(set(map(lambda x: x['SimilarityWith'], targets)))\n",
                "comparison_list = list(set(map(lambda x: x['Name'], comparison)))\n",
                "\n",
                "vectorizer = TfidfVectorizer(min_df=1, analyzer=ngrams, lowercase=False)\n",
                "tfidf = vectorizer.fit_transform(targets_list)\n",
                "nbrs = NearestNeighbors(n_neighbors=1, n_jobs=-1).fit(tfidf)\n",
                "\n",
                "def getNearestN(query):\n",
                "  queryTFIDF_ = vectorizer.transform(query)\n",
                "  distances, indices = nbrs.kneighbors(queryTFIDF_)\n",
                "  return distances, indices\n",
                "\n",
                "distances, indices = getNearestN(comparison_list)\n",
                "comparison_list = list(comparison_list)\n",
                "\n",
                "matches = []\n",
                "for i,j in enumerate(indices):\n",
                "  temp = [round(distances[i][0],2), targets_list[j[0]], comparison_list[i]]\n",
                "  matches.append(temp)"
            ],
            "metadata": {
                "azdata_cell_guid": "342ba2e9-9954-49fe-87e1-bb5df436689a",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "CPU times: total: 15.6 ms\nWall time: 22 ms\n"
                }
            ],
            "execution_count": 8
        },
        {
            "cell_type": "code",
            "source": [
                "%%time\n",
                "\n",
                "matches_df = pd.DataFrame(matches, columns=['Distance','Target name','Dataset name'])\n",
                "\n",
                "#matches_df = matches_df.loc[matches_df['Distance'] > 0.3]\n",
                "matches_df = matches_df.drop_duplicates().sort_values(\n",
                "    by=['Distance'], ascending=True)\n",
                "    \n",
                "matches_df.to_csv('tf-idf.csv', index=False)\n",
                "print(len(matches_df))"
            ],
            "metadata": {
                "azdata_cell_guid": "4f75fa7e-f71e-46d4-8869-dac3feb7d367",
                "language": "python"
            },
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": "224\nCPU times: total: 15.6 ms\nWall time: 5.02 ms\n"
                }
            ],
            "execution_count": 9
        },
        {
            "cell_type": "markdown",
            "source": [
                "Benchmarks\n",
                "\n",
                "|Approach|Search Targets Count|Dataset Names Count|CrossJoined Dataset Size| Hit Count | Score Filter | Duration |\n",
                "|--|--|--|--|--|--|--|\n",
                "|Conventional|5.000|785.000|3.925.000.000|1.257.568|80% > |~11m (12m 30s)|\n",
                "|TFIDF|5.000|785.000|-|728.326|0.3 >|56s|\n",
                "|Conventional|10.000|126.304|1.263.040.000|199.549|80% > |~3m 11s (4m 41s total)|\n",
                "|TFIDF|10.000|126.304|-|91.622|0.3 >|15.7s|\n",
                "|TFIDF|10.000|126.304|-|126.304|N/A|15s|\n",
                "|TFIDF|5.000|63.635|-|46.411|0.3 >|4.6s|\n",
                "|Conventional|5.000|63.635|318.175.000|82.011|80% >|~31s (2m 1s total)|\n",
                "\n",
                "* Spark seems to take about 1m 30s initialization time independently of the dataset size, at the computer i'm currently running the script"
            ],
            "metadata": {
                "azdata_cell_guid": "8830ad1d-74e4-48f5-bbc4-fa680cb970fe"
            }
        }
    ]
}